# Семинар 2 - Битовое представление данных

Сегодня мы будем часто смотреть на двоичное представление чиселок в памяти. Давайте научимся писать функцию, способную побитово вывести целое число в консоль:
```c
#include <stdint.h>

void print_bits_n(uint64_t num, int nbits) {
    for (int i = 0; i < nbits; i++) {
        uint64_t mask = (uint64_t)1 << (nbits - 1 - i);
        printf("%d", (num & mask) != 0);
        if ((i + 1) % 8 == 0)
            printf(" ");
    }
    printf("\n");
}
```

Если вы хотите целочисленную переменную определенного размера, то используйте `(u)int*_t` из `stdint.h`.

## Целые числа

### Представление целого числа в памяти

Целые числа делятся на знаковые и беззнаковые. С предствлением беззнаковых чисел всё просто - они хранятся в памяти в двоичном формате. С положительными знаковыми числами всё абсолютно так же. Для хранения отрицательных чисел используется two's complement: чтобы сохранить число `-x`, мы записываем такое двоичное число, что если к нему прибавить `+x`, то получится `2^nbits`, где `nbits` это количество бит в числе. Иными словами, должен получится ноль с точностью до переполнения.

### Endianness

Вопрос с подвохом: вот у меня в памяти лежит 16-битное число `0x1234` (если что, `0x` означает шестнадцатиричную запись). Если я возьму указатель на это число и прочитаю по нему один байт, то что я получу, `0x12` или `0x34`?

Подвох заключается в том, что ответ: "зависит от системы". Разные системы расставляют байты по-разному: одни в меньшие адреса пишут младшие байты (они вернут нам `0x34`) и называются Little-Endian, а другие наоборот - пишут в меньшие адреса старшие байты (они вернут нам `0x12`) и называются Big-Endian. Процессоры с архитектурами x86 и ARM, которые почти точно стоят в наших с вами устройствах, используют Little-Endian и вернут нам `0x34`, можете попробовать:

```c
#include <stdio.h>
#include <stdint.h>
#include <string.h>

int main() {
    uint16_t num16 = 0x1234;
    uint8_t num8;

    memcpy(&num8, &num16, 1);
    printf("0x%x\n", num8);

    return 0;
}
```

Но протокол IP (извините за тавтологию), на котором держится весь интернет, использует Big-Endian запись, и потому при работе с ним требуется знать об этих особенностях и пользоваться функциями типа `htonl` и `ntohl` для преобразования из одного формата в другой.

### Операции с целыми числами и Undefined Behavior

Давайте перечислим операции, которые мы знаем: `+`, `-`, `*`, `/`, `%`, `~`, `|`, `&`, `^`, `>>`, `<<`, `&&`, `||`.

#### Арифметические операции

Казалось бы, ничего сложного. Но у арифметических операций есть пара подводных камней. Взгляните на функцию
```c
int foo(int num) {
    return num + 1 > num;
}
```

Давайте прогоним её через [compiler explorer](https://godbolt.org/). Для улучшения читаемости я указал `-fomit-frame-pointer -O1`, если хотите, можете попробовать убрать эти флаги, сути это не поменяет. Вывод один, и он плачевный:
```
foo:
        mov     eax, 1
        ret
```

Функция безусловно возвращает `true`. То есть несмотря на то, что по факту может произойти переполнение и это условие не выполнится, **переполнение знаковых типов в языке C это UB**, и компилятор этим пользуется. Если же указать беззнаковый тип для `num` (например, `unsigned int`), то такого бага не появится. Для других арифметических операций (вычитание, умножение) ситуация такая же. Это зачастую неожиданное поведение, и даже у взрослых дядек [иногда от него горит](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=30475#c4).


Это весьма неприятно и неудобно в некоторых случаях, так что gcc предоставляет несколько способов с этим справиться. Например, можно полностью отключить это поведение с помощью флага `-fwrapv`, но тогда придётся распрощаться с радом оптимизаций, которые может сделать компилятор. Если же вы не хотите пессимизировать всю программу, а просто проверить на переполнение в одном месте, то вам помогут [интринзики](https://gcc.gnu.org/onlinedocs/gcc/Integer-Overflow-Builtins.html).

С их помощью можно исправить нашу функцию следующим образом:
```c
int foo(int num) {
    return !__builtin_add_overflow(num, 1, (int*)0);
}
```
или же так
```c
int foo(int num) {
    int sum;
    __builtin_add_overflow(num, 1, &sum);
    return sum < num;
}
```

Целочисленное деление на ноль это тоже UB. На практике это скорее всего просто уронит вам код, но это не гарантируется. Компилятор может применить какую-нибудь оптимизацию, которая будет опираться на то, что делитель ненулевой, обойдётся без деления и ваш код просто начнёт выдавать мусор.

#### Побитовые операции

У битовых операций тоже есть подводные камни. Самый опасный: битовый сдвиг на отрицательную длину или на длину, больше **или равную** размеру числа, это UB. Давайте я приведу пример ошибки, с которой я сталкивался в настоящем коде. Представьте, что мы хотим оставить только последние N бит у заданного числа. Для этого был написан примерно следующий код:

```c
#include <stdint.h>
uint32_t last_bits(uint32_t num, int nbits) {
    uint32_t mask = 1 << nbits; // 10000000
                                //  |<-n->|
    mask -= 1;                  // 01111111

    return num & mask;
}
```

На первый взгляд всё правильно, но вот беда, при `nbits == 32` эта функция возвращает лишь последний бит. Как же так? Потому что происходит UB.

### Логические операции

Ещё у операторов `&&` и `||` есть интересная особенность - они ленивые. Если результат операции понятен уже после вычисления левого операнда, то правый не вычисляется. Например следующая программа ничего не выведет:

```c
#include <stdio.h>

int always_true() {
    return 1;
}

int always_false() {
    return 0;
}

int print_something() {
    printf("hello\n");
    return 1;
}

int main() {
    if (always_true() || print_something()) {
    }

    if (always_false() && print_something()) {
    }

    return 0;
}
```

Ещё одно отличие этих операций от побитовых - они смотрят сразу на всё число, а не на отдельные биты. Для них `0` это ложь, а любое другое значение это истина. И возвращают они `0`, если результат - ложь, и `1` если истина. То есть `1 & 2 == 0`, но `1 && 2 == 1`, и `3 | 4 == 7`, но `3 || 4 == 1`.

## Числа с плавающей точкой

Они же числа с плавающей запятой.

### IEEE754

Это стандарт представления чисел с плавающей точкой. На википедии про него есть [приемлемая статья](https://en.wikipedia.org/wiki/IEEE_754). Де-факто, в C этот стандарт почти всюду используется и на него можно опираться. При желании вы вообще можете отказаться компилироваться при отсутствии его поддержки.

```c
#ifndef __STDC_IEC_559__
    #error "Nope, not compiling without IEE754"
#endif
```

Этот стандарт задаёт репрезентацию дробных чисел в памяти компьютера. Например, для 32-битной версии представление такое: 1 бит - знак, 8 бит - экспонента, 23 бита - мантисса. Поиграться с этим представлением вы можете [вот тут](https://www.h-schmidt.net/FloatConverter/IEEE754.html). На всякий случай отмечу, что в языке C как правило `float` это 32-битное число с плавающей точкой, а `double` это 64-битное число с плавающей точкой.

### Пытаемся вывести float

На данный момент мы умеем выводить побитовую репрезентацию целочисленного типа. Но как сделать то же самое для типа `float`?

Есть несколько способов. Все они заключаются в том, чтобы произвести *каламбур типизации* (или type-pun), побитово представив `float` в виде целочисленного типа.

#### Неправильный type-punning
Самый грубый метод это просто скастить указатель
```c
void print_bits_float(float numf) {
    uint32_t num = *(uint32_t*)(&numf);
    print_bits_n(num, 32);
}
```
Оно, конечно, сработает, но данный код это UB из-за того, что он нарушает [правила алиасинга](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fstrict-aliasing). Правила алиасинга требуют (с [небольшими исключениями](https://en.cppreference.com/w/c/language/object)), чтобы указатели разных типов не ссылались на одну и ту же область памяти. Здесь мы эти правила явно нарушаем: мы создаём указатель типа `uint32_t`, который ссылается на область памяти, где лежит `float`. Компилятор сильно опирается на эти правила при проведении оптимизаций, но если вам очень хочется, то можете передать ему флаг `-fno-strict-aliasing` и он перестанет на это опираться, и подобное UB в вашем коде не реализуется.

#### Менее неправильный type-punning
Менее грубый метод это использование `union`
```c
typedef union {
    int i;
    float f;
} punner_t;

void print_bits_float(float numf) {
    punner_t num = {.f = numf};
    print_bits_n(num.i, 32);
}
```

На семинаре я допустил неточность, сказав, что тут присутствует UB, на [самом деле нет](https://en.cppreference.com/w/c/language/union). Один умник тоже пытался доказать Линусу Торвальдсу, что так делать не надо и был [уничтожен как дешёвка](https://www.yodaiken.com/2018/06/07/torvalds-on-aliasing/).

#### "Сишный" способ
С какой-то стороны, самым правильным и общепризнанным способом было бы использование `memcpy`:

```c
#include <string.h>

void print_bits_float(float numf) {
    uint32_t num;
    memcpy(&num, &numf, 4);
    print_bits_n(num, 32);
}
```

Здесь точно нет никакого UB, и если кто-то переживает по поводу производительности - не переживайте, попробуйте вбить это в compiler explorer и увидите, что компилятор насквозь видит этот `memcpy` и успешно оптимизирует его.

#### Плюсовый способ

В качестве заметки, плюсовики тоже часто сталкиваются с проблемой алиасинга и поэтому в C++20 они ровно для наших целей добавили [std::bit_cast](https://en.cppreference.com/w/cpp/numeric/bit_cast).

#### Разбиение на поля

Ну а если вы хотите посмотреть на отдельные части репрезентации `float`'а изнутри программы, то вот вам удобная структурка:

```c
typedef struct {
    uint32_t mantissa:23;
    uint32_t exponent:8;
    uint32_t sign:1;
} fp_repr;
```

Здесь мы используем [битовые поля](https://en.cppreference.com/w/cpp/language/bit_field).
Можете точно также скаламбурить `float` в эту структурку любым удобным способом и смотреть на эти поля по отдельности.

### Мантисса, экспонента...

Если со знаковым битом всё понятно, то с мантиссой и экспонентной дела обстоят немного посложнее. Для нашего удобства я немного модифицировал функцию побитового вывода флоата, чтобы она делала более правильное его разбиение:

```c
#include <limits.h>

void print_bits_32f(float numf) {
    int nbits = sizeof(numf) * CHAR_BIT;

    uint32_t num;
    memcpy(&num, &numf, sizeof(numf));

    for (int i = 0; i < nbits; i++) {
        uint64_t mask = (uint64_t)1 << (nbits - 1 - i);
        printf("%d", (num & mask) != 0);
        if ((i + 1) == 1 || (i + 1) == 9)
            printf(" ");
    }
    printf("\n");
}
```

Давайте разбираться. Известно, что мантисса кодирует само число, а экспонента его порядок. Причём они оба делают это в двоичном виде. Давайте попробуем вывести числа `1.0 == 1.0 * 2^0`, `2.0 == 1.0 * 2^1`, `1.75 == 1.11 * 2^0`, `3.375 == 1.1011 * 2^1`, `1025.0 == 1.0000000001 * 2^10`, `0.125 == 1.0 * 2^-3`.

```c
int main() {
    float nums[] = {1.f, 2.f, 1.75f, 3.375f, 1025.f, 0.125f};
    for (int i = 0; i < sizeof(nums) / sizeof(float); i++) {
        printf("%g:\n", nums[i]);
        print_bits_32f(nums[i]);
    }
}
```

Обратите внимание на суффикс `f`, который я ставлю после чисел. По умолчанию дробные числа в языке C воспринимаются как `double`, а не как `float`. В конкретно этой ситуации это, может, и не играет большой роли, но иногда за этим нужно следить. Посмотрим же, что у нас получилось:

```
1:
0 01111111 00000000000000000000000
2:
0 10000000 00000000000000000000000
1.75:
0 01111111 11000000000000000000000
3.375:
0 10000000 10110000000000000000000
1025:
0 10001001 00000000010000000000000
0.125:
0 01111100 00000000000000000000000
```

Видите паттерн? Во-первых, в экспоненте хранится не просто степень двойки, а степень двойки плюс `0b01111111 == 255`. Во вторых, в мантиссе не записана лидирующая единичка перед точкой, а лишь знаки после неё. Действительно, если первая значащая цифра это всегда единичка, то зачем её хранить? Незачем, за небольшим исключением. Давайте возьмём число и начнём делить его на два и смотреть на его репрезентацию:

```c
int main() {
    float num = 1.5f;
    for (int i = 0; i < 200; i++) {
        printf("%g:\n", num);
        print_bits_32f(num);
        num /= 2;
    }
}
```

```
<часть вывода опущена>
0 00000100 10000000000000000000000
7.05297e-38:
0 00000011 10000000000000000000000
3.52648e-38:
0 00000010 10000000000000000000000
1.76324e-38:
0 00000001 10000000000000000000000
8.81621e-39:
0 00000000 11000000000000000000000
4.4081e-39:
0 00000000 01100000000000000000000
2.20405e-39:
0 00000000 00110000000000000000000
1.10203e-39:
0 00000000 00011000000000000000000
5.51013e-40:
0 00000000 00001100000000000000000
<и так далее>
8.40779e-45:
0 00000000 00000000000000000000110
4.2039e-45:
0 00000000 00000000000000000000011
2.8026e-45:
0 00000000 00000000000000000000010
1.4013e-45:
0 00000000 00000000000000000000001
0:
0 00000000 00000000000000000000000
<и далее нули>
```

И что мы видим: когда экспонента доходит до минимального значения (т.е. до -255), лидирующая единичка появляется, и благодаря этому можно выжать еще немного точности из формата. Это явление называется *денормализация*. Но в какой-то момент мы таки схлопываемся до нуля. Кстати, заметьте, что в данном формате `+0` и `-0` имеют разные репрезентации.

Давайте теперь будем наоборот увеличивать наше число и не делить, а умножать его на два с каждой итерацией:

```
<...>
6.38029e+37:
0 11111100 10000000000000000000000
1.27606e+38:
0 11111101 10000000000000000000000
2.55212e+38:
0 11111110 10000000000000000000000
inf:
0 11111111 00000000000000000000000
inf:
0 11111111 00000000000000000000000
<...>
```

Как мы видим, здесь нет явления, аналогичного денормализации при малых значениях - мы сразу же перетекаем в бесконечность. Как мы видим, бесконечность наступает при максимальной экспоненте и нулевой мантиссе. И аналогично нулю, существует две бесконечности: положительная и отрицательная. Причём:

```c
int main() {
    float nums[] = {
        1.f / +0.f,
        1.f / -0.f,
        0.f / 0.f,
    };
    for (int i = 0; i < sizeof(nums) / sizeof(float); i++) {
        printf("%g:\n", nums[i]);
        print_bits_32f(nums[i]);
    }
}
```

```
inf:
0 11111111 00000000000000000000000
-inf:
1 11111111 00000000000000000000000
-nan:
1 11111111 10000000000000000000000
```

Помимо бесконечности и минус бесконечности существует ещё один класс специальных значений - наны (NaN - Not a Number). Представление не-числа NaN это максимальная экспонента и ненулевая мантисса. Они возникают, когда операция не определена. Например, если поделить нули, сложить противоположные бесконечности или взять корень из отрицательного числа. Стандарт IEEE754 также делит наны на два класса: qNaN (quiet NaN - тихий нан) и sNaN (signalling NaN - сигнализирующий нан). У qNaN старший бит мантиссы должен быть выставлен, а у sNan он должен равняться нулю. Это различие можно использовать, чтобы рушить программу на сигнальных нанах, но не на тихих:

```c
#define _GNU_SOURCE
#include <fenv.h>

#include <stdint.h>
#include <string.h>
#include <stdio.h>

int main() {
    fesetenv(FE_NOMASK_ENV);

    float snan, qnan;
    uint32_t qnan_repr = 0b01111111110000000000000000000000;
    uint32_t snan_repr = 0b01111111101000000000000000000000;

    memcpy(&snan, &snan_repr, 4);
    memcpy(&qnan, &qnan_repr, 4);

    printf("%f\n", qnan);
    printf("%f\n\n", snan);
}
```

```
$ gcc nans.c -lm
$ ./a.out
nan
Floating point exception
```

### Сравниваем наны

Очень интересное свойство нанов в том, что оператор `==` строго говоря не задаёт отношение эквивалентности над числами с плавающей запятой, так как NaN != NaN.

```c
int main() {
    float nan = 0.f / 0.f;
    printf("%d\n", nan == nan); // prints "0"
}
```

### See also
Больше о тонкостях работы с числами с плавающей точкой в gcc [смотри тут](https://gcc.gnu.org/wiki/FloatingPointMath). Особо хочу обратить ваше внимание на то, что согласно iee754 погрешности в вычислениях с плавающей точкой **не случайны, а детерминированны и воспроизводимы**.

## Как вывести float в терминал?
Это удивительно сложный вопрос с простым ответом. В силу того, что IEE754 использует двоичную систему при хранении, а мы десятичную, не всегда удаётся сохранить идеальную точность. Например, десятичное `0.1` это бесконечная двоичная дробь и поэтому истиное значение, которое сохранится в 32-битный `float`, это `0.100000001490116119384765625`. Так вот когда мы хотим вывести это число в терминал, то что нам выводить? Нам выводить все эти числа? Нам выводить только несколько значащих цифр? А сколько? Может, шесть? Но ведь тогда получается, для числа `1.0000001` мы потеряем точность при выводе? А для числа `1.0` нам тоже шесть значащих цифр выводить? Можете посмотреть, как люди со stackoverflow [не могут сойтись к правильному ответу](https://stackoverflow.com/questions/554063/how-do-i-print-a-double-value-with-full-precision-using-cout).

На самом деле ответ существует и он очень прост: наиболее правильно вывести кратчайшую по количеству символов десятичную дробь в `fixed` или `scientific` формате, такую, что при считывании обратно в двоичный формат получалось ровно то же самое значение (иными словами, чтобы выполнялась round-trip guarantee) ([пруф](https://dl.acm.org/doi/pdf/10.1145/93548.93559), [видео-пруф](https://youtu.be/_frCGEC23zo)). Такой способ позволяет записывать и считывать дробные числа **без потери точности**.

Эти гарантии дают `std::to_chars` ([пруф](http://eel.is/c++draft/charconv#to.chars-2)) и `std::format` ([пруф](https://en.cppreference.com/w/cpp/utility/format/formatter)) из C++. Похожее поведение даёт `printf("%g")`, но я не нашёл пруфов, что для него выполняются нужные гарантии, но он вроде лучше, чем `%f` или `%e`.

## UTF-8 и Юникод

Как вы уже могли заметить ранее, строки в C это массивы `char`'ов, размер которых строго равен одному байту, или восьми битам. Это 256 возможных значений. Можно сделать вполне очевидное умозалючение, что 255 значений на всех не хватит. Что же делать? Более того, как вообще работает следующий код?

```
int main() {
    const char[] hello = "Привет";
    print("%s\n", hello); // prints "Привет"
}
```

Более того, если я напишу строчку из более чем 256 различных символов на разных языках, то она всё ещё будет корректно выведена.

Как оно работает с английскими символами - понятно. Есть [табличка ASCII](https://www.alpharithms.com/s3/assets/img/ascii-chart/ascii-table-alpharithms-scaled.jpg), в которой показано, какой английский символ имеет какой код.

Существует Юникод - таблица соответствия чисел тем или иным смысловым единицам (именно смысловым единицам, а не символам, так как один символ может включать в себя больше одной единицы Юникода).

В народе Юникод называют кодировкой, но это строго говоря некорректно. Существуют кодировки *Юникода*, с помощью которых последовательность code-point'ов/смысловых единиц транслируется в поток байт. Например, если просто записывать код-поинты по 4 байта на каждый, то получится кодировка [UTF-32](https://ru.wikipedia.org/wiki/UTF-32). Но тогда каждый символ, даже английский, будет занимать 4 байта. В языке C можно создавать такие символы и строки: тип называется `wchar_t`, а перед строковыми литералами UTF-32 надо [поставить заглавную L](https://en.cppreference.com/w/cpp/language/string_literal). Но обычно в строках (как в примере выше) мы используем другую кодировку - [UTF-8](https://ru.wikipedia.org/wiki/UTF-8). Это кодировка с переменным размером код-поинта. Английские символы в этой кодировке эквивалентны ASCII, а более длинные кодируются по правилам, которые можно почитать по ссылке. Из-за этого размер строчки "Привет" в байтах больше чем размер строчки "hello!":

```c
int main() {
    const char text1[] = "hello!";
    const char text2[] = "привет";

    printf("%ld %ld\n", sizeof(text1), sizeof(text2)); // prints "7 13"
}
```

Это происходит потому что каждая русская буква кодируется двумя байтами. Давайте посмотрим как быглядит русская буква "ё" в кодировках UTF-32 и UTF-8.

```c
int main() {
    print_bits_n(L'ё', 32); // prints "00000000 00000000 00000100 01010001"

    const char text[] = "ё";
    for (int i = 0; i < sizeof(text); i++)
        print_bits_n(text[i], 8); // prints "11010001 10010001 00000000"
}
```

Давайте ещё раз внимательно посмотрим, переставив пробелы: в UTF-32 это 00000000 00000000 00000**100 01010001**, в UTF-8 это 110**10001** 10**010001**. Всё сходится.

### Кракозябры

А вот пример того, [что происходило](https://unicodebook.readthedocs.io/definitions.html#mojibake) до того, как кодировка UTF-8 стала общепринятой.